{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from tensorflow import keras\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epoch=10):\n",
    "  train_csv_name = input(\"train_dataフォルダ内の、学習させるcsvファイル名を入力してください\")\n",
    "  num_epoch = int(input(\"何エポックで学習させるか入力してください(デフォルトは10)\"))\n",
    "  train_csv_df = pd.read_csv(\"train_data/{}\".format(train_csv_name))\n",
    "  X = train_csv_df.loc[:,\"preprocessed_tweet\"].values.tolist()\n",
    "  y = train_csv_df.loc[:,\"label\"].values.tolist() # 1: 好き, 0: 嫌い\n",
    "  x_train, x_test, y_train, y_test = train_test_split(X, y,train_size=0.8)\n",
    "  \n",
    "  # model_nameはここから取得(cf. https://huggingface.co/transformers/pretrained_models.html)\n",
    "  model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "\n",
    "  # テキストのリストをtransformers用の入力データに変換\n",
    "  def to_features(texts, max_length):\n",
    "      tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "      shape = (len(texts), max_length)\n",
    "      # input_idsやattention_mask, token_type_idsの説明はglossaryに記載(cf. https://huggingface.co/transformers/glossary.html)\n",
    "      input_ids = np.zeros(shape, dtype=\"int32\")\n",
    "      attention_mask = np.zeros(shape, dtype=\"int32\")\n",
    "      token_type_ids = np.zeros(shape, dtype=\"int32\")\n",
    "      for i, text in enumerate(texts):\n",
    "          try:\n",
    "              encoded_dict = tokenizer.encode_plus(text, max_length=max_length, pad_to_max_length=True)\n",
    "          except:\n",
    "              pass\n",
    "          input_ids[i] = encoded_dict[\"input_ids\"]\n",
    "          attention_mask[i] = encoded_dict[\"attention_mask\"]\n",
    "          token_type_ids[i] = encoded_dict[\"token_type_ids\"]\n",
    "      return [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "  # 単一テキストをクラス分類するモデルの構築\n",
    "  def build_model(model_name, num_classes, max_length):\n",
    "      input_shape = (max_length, )\n",
    "      input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "      attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "      token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n",
    "      bert_model = transformers.TFBertModel.from_pretrained(model_name)\n",
    "      last_hidden_state, pooler_output = bert_model(\n",
    "          input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          token_type_ids=token_type_ids\n",
    "      )\n",
    "      output = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(pooler_output)\n",
    "      model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "      model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "      return model\n",
    "\n",
    "  num_classes = 2\n",
    "  max_length = 140\n",
    "  batch_size = 10\n",
    "  epochs = num_epoch\n",
    "\n",
    "  x_train = to_features(x_train, max_length)\n",
    "  y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "  model = build_model(model_name, num_classes=num_classes, max_length=max_length)\n",
    "\n",
    "  # 訓練\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs\n",
    "  )\n",
    "\n",
    "\n",
    "  #モデル保存\n",
    "  model.save_weights(\"weights/checkpoint\")\n",
    "  # 予測\n",
    "  x_test = to_features(x_test, max_length)\n",
    "  y_test = np.asarray(y_test)\n",
    "  y_preda = model.predict(x_test)\n",
    "  y_pred = np.argmax(y_preda, axis=1)\n",
    "  print(\"Accuracy: %.5f\" % accuracy_score(y_test, y_pred))\n",
    "  print(\"TRUE\")\n",
    "  print(y_test)\n",
    "  print(\"PREDICT\")\n",
    "  print(y_pred)\n",
    "  print(\"学習が完了しました。重みデータとしてcheckpoint・index・dataの３種類がweightsフォルダに保存されます\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
